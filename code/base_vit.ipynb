{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d684913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange   # einops模块 提供 rearrange数据维度重排， reduce 数据统计， repeat 复制方式扩展维度\n",
    "\n",
    "# from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e81aabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):  # Patch Embedding + Position Embedding + Class Embedding\n",
    "    def __init__(self, image_channels=3, image_size=224, patch_size=16, dim=768, drop_ratio=0.):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.num_patches = (image_size // patch_size) ** 2  # Patch数量\n",
    "\n",
    "        self.patch_conv = nn.Conv2d(image_channels, dim, patch_size, patch_size)  # 使用卷积将图像划分成Patches\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))  # class embedding\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, self.num_patches + 1, dim))  # position embedding\n",
    "        self.dropout = nn.Dropout(drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_conv(x)\n",
    "        x = rearrange(x, \"B C H W -> B (H W) C\")\n",
    "        cls_token = torch.repeat_interleave(self.cls_token, x.shape[0], dim=0)  # (1,1,dim) -> (B,1,dim)\n",
    "        x = torch.cat([cls_token, x], dim=1)  # (B,1,dim) cat (B,num_patches,dim) --> (B,num_patches+1,dim)\n",
    "        x = x + self.pos_emb\n",
    "        return self.dropout(x)  # token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4612ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):  # Multi-Head Attention\n",
    "    def __init__(self, dim, num_heads=8, drop_ratio=0.):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)  # 使用一个Linear，计算得到qkv\n",
    "        self.dropout = nn.Dropout(drop_ratio)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B: Batch Size / P: Num of Patches / D: Dim of Patch / H: Num of Heads / d: Dim of Head\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = rearrange(qkv, \"B P (C H d) -> C B H P d\", C=3, H=self.num_heads, d=self.head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # 分离qkv\n",
    "        k = rearrange(k, \"B H P d -> B H d P\")\n",
    "        # Attention(Q, K, V ) = softmax(QKT/dk)V （T表示转置)\n",
    "        attn = torch.matmul(q, k) * self.head_dim ** -0.5  # QKT/dk\n",
    "        attn = F.softmax(attn, dim=-1)  # softmax(QKT/dk)\n",
    "        attn = self.dropout(attn)\n",
    "        x = torch.matmul(attn, v)  # softmax(QKT/dk)V\n",
    "        x = rearrange(x, \"B H P d -> B P (H d)\")\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2441bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):  # MLP\n",
    "    def __init__(self, in_dims, hidden_dims=None, drop_ratio=0.):\n",
    "        super(MLP, self).__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = in_dims * 4  # linear的hidden_dims默认为in_dims的4倍\n",
    "\n",
    "        self.fc1 = nn.Linear(in_dims, hidden_dims)\n",
    "        self.fc2 = nn.Linear(hidden_dims, in_dims)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear + GELU + Dropout + Linear + Dropout\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7fe7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderBlock(nn.Module):  # Transformer Encoder Block\n",
    "    def __init__(self, dim, num_heads=8, drop_ratio=0.):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(dim)\n",
    "        self.multiheadattn = MultiHeadAttention(dim, num_heads)\n",
    "        self.dropout = nn.Dropout(drop_ratio)\n",
    "        self.layernorm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 两次残差连接，分别在Multi-Head Attention和MLP之后\n",
    "        x0 = x\n",
    "        x = self.layernorm1(x)\n",
    "        x = self.multiheadattn(x)\n",
    "        x = self.dropout(x)\n",
    "        x1 = x + x0  # 第一次残差连接\n",
    "        x = self.layernorm2(x1)\n",
    "        x = self.mlp(x)\n",
    "        x = self.dropout(x)\n",
    "        return x + x1  # 第二次残差连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7707800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):  # MLP Head\n",
    "    def __init__(self, dim, num_classes=1000):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(dim)\n",
    "        # 对于一般数据集，此处为1层Linear; 对于ImageNet-21k数据集，此处为Linear+Tanh+Linear\n",
    "        self.mlphead = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layernorm(x)\n",
    "        cls = x[:, 0, :]  # 去除class token\n",
    "        return self.mlphead(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0229db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViT(nn.Module):  # Vision Transformer\n",
    "    def __init__(self, image_channels=3, image_size=224, num_classes=1000, patch_size=16, dim=768, num_heads=12,\n",
    "                 layers=12):\n",
    "        super(ViT, self).__init__()\n",
    "        self.embedding = Embedding(image_channels, image_size, patch_size, dim)\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[EncoderBlock(dim, num_heads) for i in range(layers)]  # encoder结构为layers(L)个Transformer Encoder Block\n",
    "        )\n",
    "        self.head = MLPHead(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_emb = self.embedding(x)\n",
    "        feature = self.encoder(x_emb)\n",
    "        return self.head(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa4ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit_base(num_classes=1000):  # ViT-Base\n",
    "    return ViT(image_channels=3, image_size=224, num_classes=num_classes, patch_size=16, dim=768, num_heads=12,\n",
    "               layers=12)\n",
    "\n",
    "\n",
    "def vit_large(num_classes=1000):  # ViT-Large\n",
    "    return ViT(image_channels=3, image_size=224, num_classes=num_classes, patch_size=16, dim=1024, num_heads=16,\n",
    "               layers=24)\n",
    "\n",
    "\n",
    "def vit_huge(num_classes=1000):  # ViT-Huge\n",
    "    return ViT(image_channels=3, image_size=224, num_classes=num_classes, patch_size=16, dim=1280, num_heads=16,\n",
    "               layers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96421272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1000])\n"
     ]
    }
   ],
   "source": [
    "images = torch.randn(8, 3, 224, 224)\n",
    "vb = vit_base()\n",
    "#vl = vit_large()\n",
    "#vh = vit_huge()\n",
    "print(vb(images).shape)\n",
    "#print(vl(images).shape)\n",
    "#print(vh(images).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ed46f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import mnist\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils as utils\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "training_data = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testing_data = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6785cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db, val_db = utils.data.random_split(training_data, [50000,10000])\n",
    "\n",
    "train_loader = DataLoader(train_db, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_db, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(testing_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88f1f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit_base(num_classes=1000):  # ViT-Base\n",
    "    return ViT(image_channels=1, image_size=28, num_classes=10, patch_size=9, dim=768, num_heads=12,\n",
    "               layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "395e6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "net = vit_base().to(device=device, dtype=torch.float32)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, weight_decay=1e-8)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f771b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6051e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs[  1/ 10] val_loss: 0.17300, val_acc: 94.550\n",
      "epochs[  2/ 10] val_loss: 0.14623, val_acc: 95.340\n",
      "epochs[  3/ 10] val_loss: 0.14138, val_acc: 95.630\n"
     ]
    }
   ],
   "source": [
    "MinTrainLoss=999\n",
    "epochs=10\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "train_acc=[]\n",
    "val_acc=[]\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    total_train_loss = []\n",
    "    total_val_loss = []\n",
    "    \n",
    "    \n",
    "    # Train \n",
    "    net.train()\n",
    "    for input_img, label in train_loader:\n",
    "        input_img = input_img.cuda()\n",
    "        label = label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_img = net(input_img)\n",
    "        loss = criterion(pred_img, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "    \n",
    "    # Verify\n",
    "    net.eval()\n",
    "    \n",
    "    current = 0\n",
    "    with torch.no_grad():\n",
    "        for train_img, train_label in train_loader:\n",
    "            train_img = train_img.cuda()\n",
    "            train_label = train_label.cuda()\n",
    "            \n",
    "            pred = net(train_img)\n",
    "            total_train_loss.append(criterion(pred, train_label).item())\n",
    "            current += (pred.argmax(1)==train_label).type(torch.float).sum().item()\n",
    "            \n",
    "    train_loss.append(np.mean(total_train_loss))\n",
    "    train_acc.append(current/500)\n",
    "    \n",
    "    \n",
    "    current = 0\n",
    "    with torch.no_grad():\n",
    "        for val_img, val_label in val_loader:\n",
    "            val_img = val_img.cuda()\n",
    "            val_label = val_label.cuda()\n",
    "            \n",
    "            pred = net(val_img)\n",
    "            total_val_loss.append(criterion(pred, val_label).item())\n",
    "            current += (pred.argmax(1)==val_label).type(torch.float).sum().item()\n",
    "            \n",
    "    val_loss.append(np.mean(total_val_loss))\n",
    "    val_acc.append(current/100)\n",
    "    \n",
    "    print(\"epochs[%3d/%3d] val_loss: %.5f, val_acc: %.3f\"%(epoch, epochs, val_loss[-1],val_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plot loss curve\n",
    "plt.plot(range(50), train_loss)\n",
    "plt.plot(range(50), val_loss)\n",
    "plt.legend([\"train\",\"val\"])\n",
    "plt.xticks(np.arange(0, 50, 5))  # 横坐标的值和步长\n",
    "plt.yticks(np.arange(0, 1, 0.1))  # 横坐标的值和步长\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"loss(100%)\")\n",
    "plt.title(\"Loss curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3424a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plot accuracy curve\n",
    "plt.plot(range(50), train_acc)\n",
    "plt.plot(range(50), val_acc)\n",
    "plt.legend([\"train\",\"val\"])\n",
    "plt.xticks(np.arange(0, 50, 5))  # 横坐标的值和步长\n",
    "plt.yticks(np.arange(90, 100, 1))  # 横坐标的值和步长\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"accuracy(100%)\")\n",
    "plt.title(\"Accuracy curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ecdf52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82fc43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceccb60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b815b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08826ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3041c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
